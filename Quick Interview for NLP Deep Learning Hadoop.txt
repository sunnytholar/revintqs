Interview for Deep Learning:
1) In Deep learning as of now I have practised classification model using estimator API present in tensorflow and Keras
2)There are 3 types of  neural networks:
Densely connected neural networks - It is used for classification & regression problems
Convolutional Neural Networks: For complex image classification text
Recurrent Neural Networks: Basically for time series forecasting
3) In DNN we have Perceptron which  consists of one or more inputs, a processor and a single output.A perceptron follow 
the feed forward model meaning inputs are sent into neuron , are processes and result in output.
4) Steps of Perception
Receive Inputs -> Weight Inputs -> Sum Inputs(activation function) -> Process and generate output
5)  For eg we have perceptron with two inputs
input x1 as 12 and input x2 as 4
Now each input before it is sent to neuron must  first be weighted i.e multiplied by number between 1 and -1
now randomly multiply x1 by 0.5 and x2 by -1 i.e 6 and -4 is now our input
Output of perceptron is generated by passing that sum through an activation function.In binary output activation function is what tells the  perceptron
whether to fire or not.Many activation function to choose from logistic , trignometric , step etc.In this example lets says activation function is sign 
of sum.if sum is positive number output is 1 and if it is negative output is -1
One more thing to consider is bias if both input is zero whatver we multiply output will be zero. So to avoid bias issue we add 1 in input
This is how single perceptron is train.now in neural network multiple perception is link together in layers.
6) 3 activation function: sigmoid, hyperbolic tangent,rectified linear unit
7) Cost functionm - Cross Entropy
8) Keras:
from keras.models import Sequential
from keras.layers import Dense
import numpy
# fix random seed for reproducibility
numpy.random.seed(7)
# load pima indians dataset
dataset = numpy.loadtxt("pima-indians-diabetes.csv", delimiter=",")
# split into input (X) and output (Y) variables
X = dataset[:,0:8]
Y = dataset[:,8]
We can piece it all together by adding each layer. The first layer has 12 neurons and expects 8 input variables. 
The second hidden layer has 8 neurons and finally, the output layer has 1 neuron to predict the class (onset of diabetes or not).
# create model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
In this case, we will use logarithmic loss, which for a binary classification problem is defined in Keras as “binary_crossentropy“. 
We will also use the efficient gradient descent algorithm “adam” for no other reason that it is an efficient default.
# Compile model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
# Fit the model
model.fit(X, Y, epochs=150, batch_size=10)

============================================

Interview for NLP:
I have worked on NLP project to find the popularity of our Free Azure product
 We started with doing data cleaning i.e use gsub and replace @ with blank, https with blanl , @amp with and
Then created corpus using nltk package in python
using corpus functions convert to lower, remove stripWhitespace , remove numbers, remove non english word if any
Stemming - Remove word which are derivations of the root word. for eg - playings , plays , played need to be replaced by played
After the text is cleaned, each tweet (technically called as a document) needs to be transformed to a collection of individual words 
and the frequency of each word across all documents/tweets will need to be counted. This is done by transforming the collection of tweets
 to a matrix like structure with the columns representing the tweets/documents and the rows representing the individual words. This structure is known as TDM 
 Next remove words who frequency is very less. Such words are called as Sparse words 
 Eariler we had 14560 comments and 8900 words.After removing Sparse words  The TDM is now reduced to 67 words which seems to be much more manageable
 and can be considered to be consisting of words which will make a difference to the model. All the other words whose individual frequency 
 was less than 2% of the total words in the corpus, were removed and words whose freq was 98 were kept
 compare with bag of words and mark the words into positive , negative and neutral.so how we do in a sentence if no of positive word is more positive and so on
 
==============================================
Hadoop:
customers <- read.csv("customers.csv",stringsAsFactors=F,header=T)
now to load data from R to hadoop.first set up hadoop environment variables:
Sys.setenv("HADOOP_CMD"...)
Sys.setenv("HADOOPSTREAMING...)
now load libraray : library(rmr2)
Now loading data to HDFS from R:
customersHDFS <- to.dfs(customers)
In  Hadoop data is stored in data node in chunk of 128MB and name node contains metadata 
Program is stored in Job & Task tracker
What is MapReduce?
It is basically a framework
Massive Parallel processing technique for processing data which is distributed on commodity cluster
I will explain it wil example suppose amozon wants to calculate total sales city wise for year 2018 in India
In MapReduce job there are 2 phases map phase & the reduce phase , so what happen giving the whole data to one person it splits the whole data into chunks
on the basis of months so we will  say each mapper gets data of each month so we have 12 mappers which gets data of each month and work on it parallel at same time.
Now 1st mapper will give the name of the city and amount of sales and write it on the index card,now take 2nd city and write city name , amount sales in another 
index card and this will be done  for each city and each mapper will do same in parallel.Now mapper job is over
Reducer will then get these piles of cards ..now we will pass these to each reducer they are responsible for .for eg we can tell reducer 1 is responsible for north city,
reducer 2 is responsible for south city , r3 for east and r4 for west .now wat R1 will do it will take all south city amunt from each mapper and summ the sales againts 
each that city and get the total sale per city.
Betweem Map And reduce we aslo have 1 more phase called as shuffle phase
https://www.youtube.com/watch?v=nDdSZzP8SD8

