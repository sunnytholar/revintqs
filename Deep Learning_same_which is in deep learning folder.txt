https://www.analyticsvidhya.com/blog/2017/05/25-must-know-terms-concepts-for-beginners-in-deep-learning/                 very important link
https://medium.com/datathings/neural-networks-and-backpropagation-explained-in-a-simple-way-f540a3611f5e

NOTE : We have completed zero to deep learning with keras.Later can listen to deep learning other videos
NOTE : So for interview we  simply need to study this sheet, have  time refer 1st link here for defination and practise coding for classification problem


Index:
1 - Tensorflow set up
5 - Perceptron
8 - More on activation functions
9 -  Cost/lost functions
10 -  Gradient Descent Backpropogation
12 - Placeholder,variables and graph defination
13 - Epoch,batch,learning rate
22 - Optimization method 
24 - refer total there are 4 files ... 2 in course and 2 in solutions refer all for handson for  classification present under zero to deep learning folder
     File names are - "4 Deep Learning Intro.ipynb" , "5 Gradient Descent.33) How to extract features from image?33) How to extract features from image?ipynb","4 Deep Learning Intro Exercises Solution.ipynb","5 Gradient Descent Exercises Solution.ipynb"
     NOte - In general all the python files are good to refer.very nice
31 -  Diff between DNN and CNN
33 - How to extract features from image?
34 - Padding
35 - Filters
36 - Stride
37 - Max and  avergae pooling
38 - weights of dnn vs cnn
41) Vanishing Gradients:
42) Stochastic gradient descent
43) Backpropagation
45) LSTM and GRU
46) Batch Normalization
47)DropOut
48) Data Augmentation
49)Hyperparameter
50) Embeding layers

1)To set up tensorflow:
Steps to install tensorflow:
	1) Set up tensorflow enviroment:,
	Go to cmd and go to the location where file tfdl_env.yml is preset
	Note - We can get this file from udemy course
	2) Run below command:
	conda env create -f tfdl_env.yml
	3) Once tensorflow enviroment is set activate the tensorflow by running below command?
	conda activate tfdeeplearning
	4) Now go to jupyter notebook and test if set up done  successfully:
	import tensorflow as tf
	hello  = tf.constant("Hello World")
	sess = tf.Session()
	print(sess.run(hello))
	NOte : If below program runs successfully it means your tensorflow set up is done successfully.
2) Basic Artifical Neutrol Network(ANN) i.e Densely connected neural networks(DNN) [Note : ANN and DNN are same]
It is used for classification & regression problems
3) Convolutional Neural Networks:
For complex image classification text
4) Recurrent Neural Networks:
Basically for time series forecasting
5) Perceptron(It is one of activate functions.Also single neuron is called as perceptron)
It consists of one or more inputs, a processor and a single output.A perceptron follow the feed forward model meaning inputs are sent into neuron , 
are processes and result in output.
Steps of Perception:
	Receive Inputs
	Weight Inputs
	Sum Inputs
Pass to activation  function.It can be anything based on requirement.eg : if some is greater then 0 pass output 1 and if sum is less  then 0 pass output as 0
Process and generate output
6) For eg we have perceptron with two inputs
input x1 as 12 and input x2 as 4
Now each input before it is sent to neuron must  first be weighted i.e multiplied by number between 1 and -1
now randomly multiply x1 by 0.5 and x2 by -1 i.e 6 and -4 is now our input
Output of perceptron is generated by passing that sum through an activation function.In binary output activation function is what tells the  perceptron
whether to fire or not.Many activation function to choose from logistic , trignometric , step etc.In this example lets says activation function is sign 
of sum.if sum is positive number output is 1 and if it is negative output is -1
One more thing to consider is bias if both input is zero whatver we multiply output will be zero. So to avoid bias issue we add 1 in input
This is how single perceptron is train.now in neural network multiple perception is link together in layers.
So mathematically perception model can be represented as
z = summation of (i = 0 to n) * w(i)x(i) + b : Here b is bias, w is weight and x is input
7)Neural Network:
We know how single perception works.In neural network many perceptrons are connected in layers
There are 3 types of layers : input , hidden and output layers.
Input Layers  : Real Values from data
Hidden : Layers between input and output
3 or more hidden layers is called deep network
Output layer : Final estimate of the output
8) More on activation functions:
We need to use different activation function they are:
Sigmoid : It is a S type curve. f(x) = 1/1+e^(-x) Here x = z = wx+b . Its value is between 0 and 1
Hyperbolic Tangent : tanh(z) : Its more important  = sinh x/ cosh x = e^x+e^(-x) / 2 for Sinh  it is minus.Its value is between -1 and 1
Relu(Rectified Linear Unit) : formula is max(0,z) . Here is z is negative it will return 0 and if positive it will return value of z
9) Cost/lost functions:
It helps us to measure how well these neurons are performing.It can help us to find how far we are from expected value
y = represent true value 
a = represent neuron's prediction
Quadratic Cost function: summation of square of (y-a) / n 
It slows down learning speed due to the way formula works. So we will use another cost function
Cross Entropy : C = (-1/n) summartion(y.ln(a) + (1-y).ln(1-a)
This cost function allows for faster learn.Larger the difference faster the neuron can learn. Learning is nothing but we corect our predictions
10) Gradient Descent Backpropogation:
Gradient Descent is an optimization algorithm for finding the minimum of a function.Its usefull here as we need to minimise the cost function
So in this we try multiple weight and final on one weight i,e best value for weights of neuron input for which cost function i.e error is less.
Now when we want to adjust the weight of all the neurons we call it as backprogation  as it takes the complete error and distribues among all the 
neurons based on desired output of each  neuron.
11) Manual Neural Network:
Manual means we will not use tensorflow framework instead do it manully using functions
Refer python file : Manual Neural Network.ipynb : Not required to check if no time
12) 3 important defination:
During the optimization process Tensorflow tunes the parameter of the model to get better fit in your train data
Placeholder : An empty node which gets data from dataframe.It is initially  empty and we need to feed actual data on which we are training our model
Variables : Changeable parameter of graph i.e weights.Variables can hold the values of weights and biases throughoout the session.Variables need to be initialized
Graph : Global variable connecting variables and placeholders to operations.Graphs are set of connectd nodes.in tensorflow each node is an operation
with posssible input that can supply some output
13)Epoch,batch,learning rate:
Batch is simply on what batch we should process our data.for eg we have 1000 records if we use batch as 100..so our model will get train 10 times.It helps to train the model
faster as instead of 1 observation  at a time it will process 100 records.Again we cant take bacth size very high as our computer should be able to handle big size and our
model should also get trained  very efficiently
Epoch - Epoch is how many time model will be trained.In above eg if we use epoch as 10 , model will  get trained 10(as 100 batch sizie) * 10.So different set of random 
data will get trained.
learning rate -  it basically  helps to reduce the cost function.Low learning  rate means  it will gradually reduce the cost and is more efficient, but we try with different
option.
So all this comes under hyperparemeter tuning
14) Tensor is a fancy work for n-dimensional array
15) while running within session we use keyword "with" which helps us not to close session explicitly
16) When we run tensorflow a default graph is created. We can also create additonal graphs seperately
17) First simple linear neural network or classic neuron/neural network:
Steps: Build a Graph , Initiate the session , Feed the data in and get output
18) There are lots of other highler level APIS(Keras,Layers etc). We will see here estimator APIS
tf.estimator.LinearClassifier : Constructs a linear classification model
tf.estimator.LinearRegressor : Constructs a linear regression model
tf.estimator.DNNClassifier : Constructs a neurol network classification model
tf.estimator.DNNRegressor : Constructs a neural network regression model
Steps: Define a list of feature columns 
Create the estimator model
Create a data input function
Call train, evaluate and prediction
19) DNN : Dense neural network
hidden_units : tells us how many layers we want and in each layer how many neurons : hidden_units=[10,10,10] 
means 3 layers and 10 neurons inside each layer
20) Overview:
   -	We calcualte single neuron by simpy finding weight , multiplying  it by x then add bias . Calculate  = weight * x + bias
   - 	Then pass this w to some activation functions(eg tanh etc)  
   -    We connect many single neurons to create a  neural network
   -    Neural network has input layer, hidden layers and output layers
   - 	Cost/lost functions are used to meausre how well our neuron is performing. Eg : Quadratic Lost function
   - 	Once we have measurement of error we need to minimise it by chosing correct value of weight and bias.We use gradient descent
		to find the optimal values
   - 	Dense layers are one which is fully connected to everyother neuronns in the next layer.Later we will see softmax layer(used for multi classification problem)
21) In gradient descent:
Learning Rate : it is speed at which model learns.
Batch size - what size of data should be taken in 1 go.smalleer - less represtive of data. larger - longer  training time.SO we need to take moderate values like 16,32,64  etc
22) Optimization method or Second order behaviour of  gradient descemt:
There should be some mechanism which helps us to keep higher learning rate in intial and as we reach to minimum eror learning rate should reduce
It basically allows us  to adjust our learning rate based off the rate of descent.Different ways to do is:
AdaGrad, RMSProp and Adam. Mostly we will use Adam
This allows us to start with larger steps and then eventually go to smaller step sizes.
Adam allows this change to happen automatically.
Hence total optimization technique  is SGD(Sophisticatic graident descent) , momentum,nesterov,AdaGrad, RMSProp and Adam.
We cant say which one is best so its always 
23) Overfitting in neural networks:
With potentially hundreds of parameters in a deep learning neural network, the possibility of overfitting is very high!
L1 and L2 regularization can be used for this.Its basically adding the penatly for larger weights in the model
24) Check deep learning code for classification problem in course python file(total there are 4 files ... 2 in course and 2 in solutions refer all)
#############Till here it was dense neural network which is used for classification and regression problem#############
24) DropOut:
It is another way to avoid overfitting( a regularisation method like l1 and l2). In this neurons are removed randomly during training.It means network doesnt rely on any particular neuron
25) Expanding data:
Artifically expand data by adding noise, tilting images, adding low white noise to sound data, etc..
This will make train data overfit and we can avoid overfitting in this way.
26) MSIT Dataset:
Classic dataset for deep learning probelms.It contains images of handwritten dataset fom 0 to 9
27) Softmax Regression:
A Softmax regression returns a list of values between 0 and 1 that add up to 1.We can use this as a list of probabilities.
So suppose we have 10 labels so it gives probality to each and finally select label with high probabilty as final result
We will use softmax as an activation function
28) Now lets start with model.Steps will be
Placeholder
Variables
Create Graph Operations
Loss function
Optimizer
Create Session and run 
29) Convolutional Neural Network:
So below learning CNN  we are going to learn below:
Tensors
DNN vs CNN
Convolutional and Filters
Padding
Pooling Layers
Review Dropout
30) What is Tensor?
It is nothin but higher dimensional array (N-Dimensional)
eg : Scalar - 3 . It is just individual digit
     Vector - [1,3,4] one dimnesional array of various scalar numbers
	 Matrix - [[3,4],[3,6],[6,7]] - list of vectors
	 Tensor - [[3,4],[3,6],[6,7]],[[3,4],[3,6],[6,7]]  Higher dimensional arrays
31) Diff between DNN and CNN:
In DNN every neuron in one layer is directly connected to every other neuron in another layer
In CNN : In convolutional layer every neuron is connected to a smaller number of a nearby units in next layer
32) So why CNN instead of DNN?
The  MSIT dataset was 28 by 28 pixels.(784 total).But most images are at least 256 by 256 pickels i.e total greater then 56k
This lead to too many parameters, unscalable to new images. Hence as more and more pickels more parameters so better use CNN
33) How to extract features from image?
The process of going from image to vector of pixels  is a simple case of feature engineering to extract features from the image.There are many other ways to extract 
features from the image.They are fourier coefficients, wavelets, color histogram etc.All this take image as an input and return vector of number which we can be 
feed into our  model.But all other methods is little complex and require a domain expertise.So we need something to automatically detect local patterns which 
is nothing but a convolutions.
34) Padding:
We run into possible issue for edge neurons.There may not be input for them.We can fix this by adding a padding of zero around the image.
in simple terms when we run filter we miss the edges  information so we add 0 in the  images to avoid loosing the information. 
35) Filters
Convolution is a general purpose filter effect for images. ? Is a matrix applied to an image and a mathematical operation.
 comprised of integers. ? It works by determining the value of a central pixel by adding the. weighted values of all its neighbors together.
A filter in a CNN is like a weight matrix with which we multiply a part of the input image to generate a convoluted output. 
Let’s assume we have an image of size 28*28. We randomly assign a filter of size 3*3, which is then multiplied with different 3*3 sections of the image to form 
what is known as a convoluted output. The filter size is generally smaller than the original image size. 
The filter values are updated like weight values during backpropagation for cost minimization.
36)Stride:
 The amount by which the filter shifts is the stride. In that case, the stride was implicitly set at 1. 
 Stride is normally set in a way so that the output volume is an integer and not a fraction.
 Usually after stride our image size reduces.
37)Max and  avergae pooling:
It is used to reduce the size of the  image by discarding some information and keeping relevant information.
foreg - max pooling takes the maximum value of the patch and stores it in the new  image by discarding the values  in the other  pickles.
So pooling patches actually dont overlap so size of the image actually reduces
Similarly average pooling layer is  same  thing just instead of using max we use average
38)weights of dnn vs cnn :
weights in cnn is less then weight in fully connected network as if we have 100 input we will have 100 weights but in CBB suppose 2*2 filter
then we will  have 20 filters so 20 weights
39) Subsampling or Pooling Layers:
Pooling layers will subsample the input image, which reduces the memory use and computer load as well as reducing the number of parameters.
The way poolin works is we create 2 by 2 pool of pixels thats  known as kernel  and evaluate the maximum value .It can be of two by two or any dimensional
depending upon the size of data.Only the max value make it to next layer.Here stride is 2 .So this pooling layer will end up removing a lot of 
information even a small pooling kernel of 2 by 2 with a stride of 2 will remove 75 percent of input data.
This is most commonly and important steps of convolutional neural networks

##################From below  began  with recurrent neural networks
38) Recurrent Neural Networks:
It is basically used to solve problems that deal with sequence information.
Usally eg is of time series data, sentences so even natural language has a sequence to it.So there going to be words in a sentence in a particular order,
Audio and music in particular at time ordering 
39) In normal neuron in feed forward network
Input is given to Aggregation of inputs then to activation function then output
But in Recurrent neuron Input - Aggregation of inputs - Activation function then again back to input i.e output goes back to the input of same neuron
40)Different types of input output can be are:
sequence to sequence - Sequence input to sequence output 
eg : passing  in a set of time series information such as a year's worth of daily sales data.. expecting back a sequence of that same sales
data shifted over a certain time period into future.
Sequence input vector output:
eg : sentiment scores ..so we can feed in a sequence of words may be a paragraph of a moview review and then request back a vector indicating
whether it was a positive sentiment such as they really liked the movie. so its indicated by 1 they liked movie versus negative 1 or 0 
means not liked the movie 
Vector to sequence:
here we feed in a vector input so a single input at just a first time and then passes zeros for the rest of your time steps and then let the 
out be a sequence . So here we can pass  a single image and expect sequence of words describing the image that would be a capton
so if you might have read any research you miht have seen passing image and neural network tells a person is standing on the beach and so on
41) Vanishing Gradients:
Vanishing Gradient Problem – Vanishing gradient problem arises in cases where the gradient of the activation function is very small. 
During back propagation when the weights are multiplied with these low gradients, they tend to become very small and “vanish” as they go
 further deep in the network. This makes the neural network to forget the long range dependency. This generally becomes a problem 
in cases of recurrent neural networks where long term dependencies are very important for the network to remember.
There are few challenges for analyzing larger time series data sets with recurrent neural networks and one of those key problems 
is vanishing gradients issue
In machine learning, the vanishing gradient problem is a difficulty found in training artificial neural networks with gradient-based learning methods
 and backpropagation. In such methods, each of the neural network's weights receives an update proportional to the gradient of the error function with
 respect to the current weight in each iteration of training. The problem is that in some cases, the gradient will be vanishingly small, effectively 
 preventing the weight from changing its value. In the worst case, this may completely stop the neural network from further training.
This can be solved by using activation functions like ReLu which do not have small gradients.
42) Stochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, 
is a stochastic approximation of the gradient descent optimization and iterative method for minimizing an objective function that is written as a
 sum of differentiable functions. In other words, SGD tries to find minima or maxima by iteration.
43) Backpropagation
Backpropagation is a method used in artificial neural networks to calculate a gradient that is needed in the calculation of the weights 
to be used in the network.[1] It is commonly used to train deep neural networks, [2] a term referring to neural networks with more than one hidden layer.
44) Diff ways to avoid vanishing problem is using different activations functions , bacth normalizations etc
45) LSTM and GRU:
Long Short term memory neurons and gated recurrent units.Many of solution provided for vanishing can also apply for RNN.
It has issue because of lenth of time series input these could slow down training.A possibel solution will be to shorten time steps but this makes
mode worse at predicting longer trends.so we dont want to sacrifice shortening our input 
Another issue rnn face is thzat after while network will begin to foreget first input as information is lost at each step going through the RNN
We need some sort of long term memory for our networks.So LST was introduced to resolve this issue
NEED to do google and learn more
Optimization technique:
46) Batch Normalization:
It is a powerful technique to regularize our model.It reduces the overfitting by rescaling the features between one layer to  the next layer in deep neural network.
Hence increases accuracy
As a concept, batch normalization can be considered as a dam we have set as specific checkpoints in a river. This is done to ensure that distribution of data is the same
 as the next layer hoped to get. 
When we are training the neural network, the weights are changed after each step of gradient descent. This changes the how the shape of data is sent to the next layer.
But the next layer was expecting the distribution similar to what it had previously seen. So we explicitly normalize the data before sending it to the next layer.
47)DropOut:
It is a form of regularization used to avoid overfitting.During training, units are randomly dropped, along with their connections.
This we do because other nodes should not be very much depenedent at specific node to correct the error as nodes can randomly disables at any time.
Mostly used for large networks.Here in code 0.2, 0.4 means percentage of units to be drop
48) Data Augmentation:
It is the technique used to generate more data when we cannot collect data as sometime it is very  expensive to collect data  with label.
Its basically data transformation for image dat eg-rotating the picture, moving it little right , removing pickels at corner etc
Doing this we generate more labelled data.
49)Hyperparameter:
Its very important so refer this video again  from  zero to deep learning with keras
It is simply how to take the best value of different parameters.
There are 3 types - Random Search, Grid Search and Bayesian optimization
50) Embeding layers:
It is used in text to combine  similiar meaning of words together by using synonyms.
51)Exploding Gradient Problem – 
This is the exact opposite of the vanishing gradient problem, where the gradient of the activation function is too large. 
During back propagation, it makes the weight of a particular node very high with respect to the others rendering them insignificant. 
This can be easily solved by clipping the gradient so that it doesn’t exceed a certain value.

